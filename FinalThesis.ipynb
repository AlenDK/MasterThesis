{"cells":[{"cell_type":"markdown","metadata":{"id":"mxaVCWrwFr7j"},"source":["# Dependancies\n","\n","This section sets up imports and users data location"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kLEi3_9uF_3b"},"outputs":[],"source":["#Set up drive/folder. \n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MdCIRg3mXqF-"},"outputs":[],"source":["## Loading imports\n","%matplotlib inline\n","import matplotlib\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import glob\n","import os\n","import pickle\n","import torch\n","\n","from keras.layers import Input, Lambda, Dense, Flatten\n","from keras.models import Model\n","from keras.applications.vgg16 import VGG16\n","from keras.applications.vgg16 import preprocess_input\n","from keras.preprocessing import image\n","from keras.preprocessing.image import ImageDataGenerator\n","import keras.activations\n","from tensorflow import keras\n","from keras.models import Sequential\n","from keras.layers.convolutional import Conv2D, MaxPooling2D\n","from keras.layers import Dense, Flatten, Dropout, MaxPool2D\n","\n","from torch.autograd import Variable\n","from IPython.display import clear_output\n","from skimage.io import imread\n","from skimage.transform import resize\n","from skimage.filters import gaussian\n","from skimage.util import random_noise\n","from sklearn.model_selection import train_test_split\n","\n","#\n","import sys\n","#Append your system path\n","sys.path.append(os.path.join('drive/My Drive/'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TlExMnxJGgxt"},"outputs":[],"source":["#Define your driver\n","drive_path = 'drive/My Drive/'\n","#Define image size\n","size = 150\n","\n","#Check for Cuda\n","use_cuda = torch.cuda.is_available()\n","print(use_cuda)"]},{"cell_type":"markdown","metadata":{"id":"pRNcFHoszbi1"},"source":["# Classes and functions\n","This section defines every Class and function used\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EO51hWsjUwD2"},"outputs":[],"source":["#Functions for image augmentation\n","\n","def flip_image(image):\n","  return(np.fliplr(image))\n","\n","def image_blur(image):\n","  sigma = np.random.uniform(0.7, 1.5)\n","  return gaussian(image,sigma=sigma,multichannel=True)\n","\n","def image_noise(image):\n","  sigma = np.random.uniform(0.05, 0.8)\n","  return random_noise(image,var=sigma**2)\n","\n","def image_high_brightness(image):\n","  randomBrightness = np.random.uniform(50, 100)\n","  return image + (randomBrightness/255)\n","\n","def high_contrast(image):\n","  randomContrast = np.random.uniform(1.1, 2)\n","  return image * randomContrast\n","\n","def low_contrast(image):\n","  randomContrast = np.random.uniform(0.5, 0.9)\n","  return image * randomContrast"]},{"cell_type":"markdown","metadata":{"id":"qshfpQcAn7nT"},"source":["## Datebase"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VR86mc2O1Xno"},"outputs":[],"source":["class Database():\n","    #Takes paths for pothole and road as input\n","    def __init__(self, pothole_path, road_path, image_shape=(size, size)):\n","\n","        self.image_shape = image_shape\n","\n","        #Start of creating the main data by combinng and shuffling all data\n","        imagesR, labelsR, imagesP, labelsP = self._combineData(pothole_path, road_path)\n","\n","        print(\"Total Observations:\\t\", len(imagesR+imagesP))\n","\n","        self.imagesR = imagesR\n","        self.labelsR = labelsR\n","        self.imagesP = imagesP\n","        self.labelsP = labelsP\n","\n","    @classmethod\n","    def split_data(self, split_percantage, test_size, imagesR, labelsR, imagesP, labelsP):\n","      #Setting different lengths to know where to split the data for training, test and vali\n","      \n","      imagesPs = imagesP[:len(imagesP)-test_size]\n","\n","      size = round((len(imagesPs)*split_percantage))\n","\n","      images = imagesR[:size] + imagesP[:size] \n","      labels = labelsR[:size] + labelsP[:size] \n","\n","      restImages = imagesR[size:len(imagesR)] + imagesP[size:len(imagesP)] \n","      restlabels = labelsR[size:len(labelsR)] + labelsP[size:len(labelsP)] \n","\n","      seed = 68\n","      np.random.seed(seed)\n","      np.random.shuffle(images)\n","      np.random.seed(seed)\n","      np.random.shuffle(labels)\n","      np.random.seed(seed)\n","      np.random.shuffle(restImages)\n","      np.random.seed(seed)\n","      np.random.shuffle(restlabels)\n","\n","\n","      test_max = len(restImages) - test_size\n","      max = len(restImages)\n","\n","      aug_img, aug_label = self._add_augmentation(labels, images)\n","\n","      #Use augmented images\n","      self.train = self._format_arrays(aug_img, aug_label)\n","\n","      #Do not use augmented images     \n","      #self.train = self._format_arrays(images, labels)\n","        \n","      self.val = self._format_arrays(restImages[:test_max], restlabels[:test_max])\n","      \n","      self.test = self._format_arrays(restImages[test_max:max], restlabels[test_max:max])\n","\n","    def _format_arrays(ar, ar2):\n","      data = dict()\n","\n","      img_tot_shp = tuple([len(ar)] + list(ar[0].shape))\n","      \n","      data['images'] = np.zeros(img_tot_shp, dtype='float32')\n","      data['ids'] = np.zeros((len(ar),), dtype='int32')\n","      data['labels'] = np.zeros((len(ar),), dtype='str')\n","\n","      for i, x in enumerate(ar):\n","        try:\n","          if(ar2[i] == ''):\n","            print(\"error\")\n","          else:\n","            data['images'][i] = x \n","            data['ids'][i] = i\n","            if(ar2[i] == ''):\n","              ar2[i] = '0'\n","            data['labels'][i] = ar2[i]\n","        except Exception as e:\n","          pass        \n","\n","      data['images'] = data['images']  \n","      return data     \n","      \n","    @classmethod\n","    def add_lists(self, rimages, rlabes, images, labels):\n","      self.images = rimages + images\n","      self.labels = rlabes + labels\n","      print(\"Total Observations:\\t\", len(self.images))\n","\n","    def _combineData(self, pot_path, road_path):\n","      #Create new arrays for images and labels\n","      image_road_image = [];\n","      image_road_label = [];\n","\n","      image_hole_image = [];\n","      image_hole_label = [];\n","\n","      #We label clean roads with 0 and potholes with 1\n","      for image in pot_path:\n","        image = imread(image)\n","        image = resize(image, output_shape=self.image_shape, mode='reflect', anti_aliasing=True)\n","        image_hole_image.append(image)\n","        image_hole_label.append('1')\n","      \n","      for image in road_path:\n","        image = imread(image)\n","        image = resize(image, output_shape=self.image_shape, mode='reflect', anti_aliasing=True)\n","        image_road_image.append(image)\n","        image_road_label.append('0')\n","\n","      #Informartion about the input Data. We should expect around the same amount for each. \n","      print(\"Information on Data\")\n","      print(\"Images of normal roads:\", len(image_road_image))\n","      print(\"Images of pothole roads:\", len(image_hole_image))\n","\n","      #Combine the data\n","      imagesR = image_road_image\n","      labelsR = image_road_label\n","      imagesP = image_hole_image\n","      labelsP = image_hole_label\n","\n","      print(\"Shuffling the Data\")\n","      #Shuffling the datasets for future splitting\n","      seed = np.random.randint(0, 10000)\n","      np.random.seed(seed)\n","      np.random.shuffle(imagesR)\n","      np.random.seed(seed)\n","      np.random.shuffle(labelsR)\n","      np.random.seed(seed)\n","      np.random.shuffle(imagesP)\n","      np.random.seed(seed)\n","      np.random.shuffle(labelsP)\n","\n","      return imagesR, labelsR, imagesP, labelsP\n","\n","    def _add_augmentation(labels, images):\n","      images_array = []\n","      labels_array = []\n","\n","      maxImages = len(images);\n","\n","      for i in range(maxImages):\n","\n","        #Original Image\n","        images_array.append(images[i])\n","        labels_array.append(labels[i])\n","        \n","        #Flipped Image\n","        images_array.append(flip_image(images[i]))\n","        labels_array.append(labels[i])\n","        \n","        #Blurred Image\n","        images_array.append(image_blur(images[i]))\n","        labels_array.append(labels[i])\n","\n","        #Noise Image\n","        images_array.append(image_noise(images[i]))\n","        labels_array.append(labels[i])\n","        \n","        #Bright Image\n","        images_array.append(image_high_brightness(images[i]))\n","        labels_array.append(labels[i])\n","        \n","        #Contrast Image\n","        images_array.append(high_contrast(images[i]))\n","        labels_array.append(labels[i])\n","\n","      #Shuffling the datasets for future splitting\n","      seed = np.random.randint(0, 99911)\n","      np.random.seed(seed)\n","      np.random.shuffle(images_array)\n","      np.random.seed(seed)\n","      np.random.shuffle(labels_array)\n","\n","      #Return images and labels\n","      return images_array, labels_array"]},{"cell_type":"markdown","metadata":{"id":"KX9GMIwGKElE"},"source":["# Preparation of data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x3UhEDCdKJ-U"},"outputs":[],"source":["#Get image path for folders:\n","\n","youtube_road = glob.glob(\"drive/My Drive/Thesis- Anomaly Detection/yt_roads/*\")\n","youtube_pot =  glob.glob(\"drive/My Drive/Thesis- Anomaly Detection/yt_pot/*\")\n","\n","zoom_road = glob.glob(\"drive/My Drive/Thesis- Anomaly Detection/normal_zoom/*.jpg\")\n","zoom_pot =  glob.glob(\"drive/My Drive/Thesis- Anomaly Detection/pothole_zoom/*.jpg\")\n","\n","newPot = glob.glob(\"drive/My Drive/Thesis- Anomaly Detection/newPot/*.jpg\")\n","newRoad = glob.glob(\"drive/My Drive/Thesis- Anomaly Detection/newRoad/*.jpg\")\n","\n","#Combine folders\n","newestP = youtube_pot + zoom_pot + newPot\n","newestN = youtube_road + zoom_road + newRoad\n","\n","#Get the size of each. Should be around equal for best results. \n","print(\"Total Observations:\\t\", len(newestP))\n","print(\"Total Observations:\\t\", len(newestN))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q1bauG9x6akq"},"outputs":[],"source":["#If you already have Data you can load it here. \n","#If you dont have skip this, run the next code field and the come back and save it for next time!!!\n","\n","Save = 1\n","Load = 1\n","\n","# # Dump the data into a pickle file\n","if(Save == 1 ):\n","  if(Load == 0):\n","    with open(drive_path + 'Thesis_Anomaly_Detection/data_150_real.pickle', 'wb') as f:\n","      pickle.dump(data, f)\n","  else:\n","    # # Load the data from a pickle file\n","    with open(drive_path + 'Thesis_Anomaly_Detection/data_150_real.pickle', 'rb') as f:\n","      data = pickle.load(f)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ikMH-NX4PYY"},"outputs":[],"source":["#Create Data \n","\n","#data = Database(newestP, newestN)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ai4NaDiAOx01"},"outputs":[],"source":["#Split data\n","data.split_data(0.7, 150, data.imagesR, data.labelsR, data.imagesP, data.labelsP)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_yc7UPEwMx3e"},"outputs":[],"source":["#Define channels and size\n","nchannels, rows, cols = 3, size, size\n","#Delete faulty entries\n","result = np.where(data.train['labels'] == '')\n","data.train['labels'][result[0]] = '0'\n","\n","result = np.where(data.test['labels'] == '')\n","data.test['labels'][result[0]] = '0'\n","\n","result = np.where(data.val['labels'] == '')\n","data.val['labels'][result[0]] = '0'\n","\n","#Reshape images and get labels.\n","x_train = data.train['images'].reshape((-1, nchannels, rows, cols))\n","targets_train = data.train['labels'].astype('int32')\n","\n","x_valid = data.val['images'].reshape((-1, nchannels, rows, cols))\n","targets_valid = data.val['labels'].astype('int32')\n","\n","x_test = data.test['images'].reshape((-1, nchannels, rows, cols))\n","targets_test = data.test['labels'].astype('int32')\n","\n","print(\"Information on dataset\")\n","print(\"x_train\", x_train.shape)\n","print(\"targets_train\", targets_train.shape)\n","print(\"x_valid\", x_valid.shape)\n","print(\"targets_valid\", targets_valid.shape)\n","print(\"x_test\", x_test.shape)\n","print(\"targets_test\", targets_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"rSzo8xYcoig8"},"source":["# Keras"]},{"cell_type":"markdown","metadata":{"id":"DhICZuuY0CTS"},"source":["## Original LeNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ehBmxsbX0Cg9"},"outputs":[],"source":["#Original LeNEt\n","act = 'relu'\n","\n","model = Sequential()\n","#Layer 1\n","model.add(Conv2D(filters=6, kernel_size=(5,5), activation=act, input_shape=(150, 150, 3)))\n","model.add(MaxPooling2D(2,2))\n","#Layer 2\n","model.add(Conv2D(filters=16, kernel_size=(5,5), activation=act))\n","model.add(MaxPooling2D(2,2))\n","#Layer 3\n","model.add(Flatten())\n","model.add(Dense(120, activation=act))\n","#Layer 4\n","model.add(Dense(84, activation=act))\n","model.add(Dense(2, activation='sigmoid'))"]},{"cell_type":"markdown","metadata":{"id":"-oS9vCWK3DIb"},"source":["## HA LeNet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MFwQMpvJ3DS-"},"outputs":[],"source":["#HA LeNet\n","act = 'relu'\n","\n","model = Sequential()\n","#Layer 1\n","model.add(Conv2D(filters=32, kernel_size=(3,3), activation= act, input_shape=(size, size, 3)))\n","model.add(MaxPooling2D(2,2))\n","#Layer 2\n","model.add(Conv2D(filters=64, kernel_size=(3,3), activation= act))\n","model.add(MaxPooling2D(2,2))\n","\n","model.add(Conv2D(filters=128, kernel_size=(3,3), activation= act))\n","model.add(MaxPooling2D(2,2))\n","\n","#Layer 3\n","model.add(Flatten())\n","model.add(Dropout(0.4))\n","#Layer 4\n","model.add(Dense(512, activation=act))\n","model.add(Dense(2, activation='sigmoid'))"]},{"cell_type":"markdown","metadata":{"id":"1kBdxz6h14-x"},"source":["## Original VGG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Nkzs0fa16qS"},"outputs":[],"source":["#Original VGG\n","act = \"relu\"\n","\n","model = Sequential()\n","\n","model.add(Conv2D(64, (3,3), activation = act, padding='same', input_shape=(150,150,3)))\n","model.add(Conv2D(64, (3,3), activation = act, padding='same'))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Conv2D(128, (3, 3), activation=act,padding='same')) \n","model.add(Conv2D(128, (3, 3), activation=act,padding='same')) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(256, (3, 3), activation=act,padding='same')) \n","model.add(Conv2D(256, (3, 3), activation=act,padding='same')) \n","model.add(Conv2D(256, (3, 3), activation=act,padding='same')) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(512, (3, 3), activation=act,padding='same'))\n","model.add(Conv2D(512, (3, 3), activation=act,padding='same')) \n","model.add(Conv2D(512, (3, 3), activation=act,padding='same')) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(512, (3, 3), activation=act,padding='same')) \n","model.add(Conv2D(512, (3, 3), activation=act,padding='same')) \n","model.add(Conv2D(512, (3, 3), activation=act,padding='same')) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Flatten())\n","\n","model.add(Dense(256, activation=act))\n","model.add(Dense(128, activation=act))\n","model.add(Dense(2, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"GuCUqDrO187w"},"source":["## HA VGG"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htn3TWwirk7j"},"outputs":[],"source":["#HA VGG\n","act = \"relu\"\n","\n","model = Sequential()\n","\n","model.add(Conv2D(64, (3,3), activation = act, padding='same', input_shape=(150,150,3)))\n","model.add(Conv2D(64, (3,3), activation = act, padding='same'))\n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Conv2D(128, (3, 3), activation=act,padding='same')) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(256, (3, 3), activation=act,padding='same')) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(512, (3, 3), activation=act,padding='same'))\n","\n","model.add(Flatten())\n","\n","model.add(Dropout(0.6))\n","\n","model.add(Dense(128, activation=act))\n","model.add(Dense(2, activation='softmax'))"]},{"cell_type":"markdown","metadata":{"id":"7pkFh48HrSmi"},"source":["## HACNN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1p1zaEWaYkKC"},"outputs":[],"source":["#HACNN\n","act = \"relu\"\n","\n","model = Sequential()\n","\n","model.add(Conv2D(64, (4,4), activation = act, input_shape=(150,150,3)))\n","model.add(Conv2D(64, (3, 3), activation=act)) \n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Conv2D(64, (3, 3), activation=act)) \n","model.add(Conv2D(64, (3, 3), activation=act)) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(128, (3, 3), activation=act)) \n","model.add(Conv2D(128, (3, 3), activation=act)) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(128, (3, 3), activation=act)) \n","model.add(MaxPooling2D((1, 1)))\n","\n","model.add(Conv2D(64, (2, 2), activation=act)) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(32, (2, 2), activation=act)) \n","\n","model.add(Flatten())\n","\n","model.add(Dropout(0.6))\n","\n","model.add(Dense(500, activation=act))\n","\n","model.add(Dense(2, activation='sigmoid'))"]},{"cell_type":"markdown","source":["## Final Model"],"metadata":{"id":"ZiqHA8Lu9n98"}},{"cell_type":"code","source":["#Final Model\n","act = \"relu\"\n","\n","model = Sequential()\n","\n","model.add(Conv2D(32, (2,2), activation = act, input_shape=(150,150,3)))\n","model.add(Conv2D(64, (2, 2), activation=act)) \n","model.add(MaxPooling2D((2,2)))\n","\n","model.add(Conv2D(32, (2, 2), activation=act)) \n","model.add(Conv2D(64, (2, 2), activation=act)) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(128, (2, 2), activation=act))\n","model.add(Conv2D(128, (2, 2), activation=act)) \n","model.add(MaxPooling2D((2, 2)))\n","\n","model.add(Conv2D(256, (2, 2), activation=act))\n","model.add(Conv2D(256, (2, 2), activation=act)) \n","model.add(MaxPooling2D((2, 2)))\n","\n","\n","model.add(Conv2D(32, (2, 2), activation=act)) \n","\n","model.add(Flatten())\n","\n","model.add(Dense(677, activation=act))\n","\n","model.add(Dropout(0.4))\n","\n","model.add(Dense(2, activation='sigmoid'))"],"metadata":{"id":"D7mZtAvo9po8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RxkOcaO2rnbu"},"source":["## Model Trainning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dwmEsJ6mkp95"},"outputs":[],"source":["lr = 0.00007\n","\n","opt = keras.optimizers.Adam(learning_rate=lr)\n","\n","#opt = keras.optimizers.SGD(lr=0.1, momentum=0.0, nesterov=False)  # keras.optimizers.SGD(lr=lr, momentum=0.9)\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lNpraQt80eri"},"outputs":[],"source":["callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=7)\n","\n","tf.config.run_functions_eagerly(True)\n","x_train = data.train['images'].reshape((-1, cols, rows, nchannels))\n","targets_train = data.train['labels'].astype('int32')\n","\n","x_valid = data.val['images'].reshape((-1, cols, rows, nchannels))\n","targets_valid = data.val['labels'].astype('int32')\n","\n","training_history=model.fit(x_train, targets_train, epochs=1, batch_size=64, verbose=2,  callbacks=[callback], #callbacks=callbacks_list,\n","         validation_data=(x_valid, targets_valid))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cu_ON_Kz0vYJ"},"outputs":[],"source":["x_test = data.test['images'].reshape((-1, cols, rows, nchannels))\n","y_test = data.test['labels'].astype('int32')\n","\n","prediction = model.predict(x_test)\n","\n","y_pred = np.argmax(prediction, axis=1)\n","#Transform predictions into 1D array \n","\n","# Making predictions on test data\n","prediction = model.predict(x_test)\n","#Transform predictions into 1D array \n","y_pred = np.argmax(prediction, axis=1)\n","\n","y_test1=y_test\n","y_test2=[]\n","for i in y_test1:\n","  a=i\n","  y_test2.append(a)  \n","\n","from sklearn.metrics import confusion_matrix\n","cm = confusion_matrix(y_test2, y_pred)\n","print(cm)\n","\n","TN = cm[0][0] #This was TP, but we want to check if its pothole or not\n","FN = cm[0][1] #This was FP, but we changed\n","FP = cm[1][0]\n","TP = cm[1][1]\n","print('True positive = ', TP)\n","print('False positive = ', FP)\n","print('False negative = ', FN)\n","print('True negative = ', TN)\n","\n","Accuracy = (TP+TN)/(TP+FP+TN+FN)\n","Precision = TP / (TP+FP)\n","Recall = TP / (TP+FN)\n","Specificity = TN / (TN+FP)\n","\n","F1 = (2*Precision*Recall) / (Precision+Recall)\n","\n","print('Accuracy = ', Accuracy)\n","print('Precision = ', Precision)\n","print('Recall = ', Recall)\n","print('Specificity = ', Specificity)\n","print('F1 = ', F1)"]},{"cell_type":"markdown","metadata":{"id":"xiaoG_ePrXB8"},"source":["## Print code"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KYzJJmuwe58o"},"outputs":[],"source":["#Run this to print code\n","def render_training_history(training_history):\n","    loss = training_history.history['loss']\n","    val_loss = training_history.history['val_loss']\n","\n","    accuracy = training_history.history['accuracy']\n","    val_accuracy = training_history.history['val_accuracy']\n","\n","    plt.figure(figsize=(14, 4))\n","\n","    plt.subplot(1, 2, 1)\n","    plt.title('Loss')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Loss')\n","    plt.plot(loss, label='Training set')\n","    plt.plot(val_loss, label='Test set', linestyle='--')\n","    plt.legend()\n","    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n","\n","    plt.subplot(1, 2, 2)\n","    plt.title('Accuracy')\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Accuracy')\n","    plt.plot(accuracy, label='Training set')\n","    plt.plot(val_accuracy, label='Test set', linestyle='--')\n","    plt.legend()\n","    plt.grid(linestyle='--', linewidth=1, alpha=0.5)\n","\n","    plt.show()"]},{"cell_type":"markdown","source":["## Visualize layers"],"metadata":{"id":"YyHGqDgI9Viv"}},{"cell_type":"code","source":["def visualize_conv_layer(layer_name):\n","  \n","  layer_output=model.get_layer(layer_name).output  #get the Output of the Layer\n","  intermediate_model=tf.keras.models.Model(inputs=model.input,outputs=layer_output) #Intermediate model between Input Layer and Output Layer which we are concerned about\n","  intermediate_prediction=intermediate_model.predict(x_train[510].reshape(1, 150, 150, 3)) #predicting in the Intermediate Node\n","  \n","  row_size=4\n","  col_size=8\n","  \n","  img_index=0\n","  print(np.shape(intermediate_prediction))\n","    #---------------We will subplot the Output of the layer which will be the layer_name----------------------------------#\n","  \n","  fig,ax=plt.subplots(row_size,col_size,figsize=(10,8)) \n","  for row in range(0,row_size):\n","    for col in range(0,col_size):\n","      ax[row][col].imshow(intermediate_prediction[0, :, :, img_index], cmap='gray')\n","      img_index=img_index+1 #Increment the Index number of img_index variable\n","        \n","print(\"Function to Visualize the Output has been Created\")"],"metadata":{"id":"Prkl721P9YR3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"9lq16pAL9a5h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#The parameter to this function is the layer name from model.summary\n","visualize_conv_layer('conv2d_9')"],"metadata":{"id":"LfIVI_9E9ZHy"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["-oS9vCWK3DIb","1kBdxz6h14-x","GuCUqDrO187w","7pkFh48HrSmi"],"machine_shape":"hm","provenance":[{"file_id":"1q4u3n022qqb6cPivTDJcyBJ8M-Bf7XQM","timestamp":1675799552963}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}